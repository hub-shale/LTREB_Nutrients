---
title: "Metadata - PAR - Judith River Watershed - FS2021"
---

```{r setup, include=FALSE}

# Do not echo code chunks in rendered document

knitr::opts_chunk$set(echo = FALSE);

# Do not render values returned by code in code chunks

knitr::opts_chunk$set(results = "hide")

# Clear the global environment

rm(list = ls());

# Load the tidyr package for use of the pipe operator

library(tidyr)

```

<!-- Automatically populate a description of the compilation of this file. -->

Generated from R markdown file **./02_protocol/`r knitr::current_input()`**, last compiled on `r Sys.time()`


```{r}

# Make sure the target directories exist.

tempPath <- "./03_incremental/temp";
dir.create(
  path = tempPath,
  recursive = TRUE,
  showWarnings = FALSE
)

emlalPath <- "./03_incremental/temp/EML";
dir.create(
  path = emlalPath,
  recursive = TRUE,
  showWarnings = FALSE
)

productPath <- "./04_product";
dir.create(
  path = productPath,
  recursive = TRUE,
  showWarnings = FALSE
)
```

```{r}
# Load the metadata from each site

llmetaEnv <- readRDS(file = "./01_input/JRWLLPAR01_20210304_PAR_v20220801/03_incremental/temp/ll_metadata.RData")

pcmetaEnv <- readRDS(file = "./01_input/JRWPCPAR02_20210604_PAR_v20220801/03_incremental/temp/pc_metadata.RData")

rrmetaEnv <- readRDS(file = "./01_input/JRWRRPAR02_20210304_PAR_v20220801/03_incremental/temp/rr_metadata.RData")

```


```{r}
# Extract overall start and end times across sites

envsT <- c(llmetaEnv$startTime, llmetaEnv$endTime, pcmetaEnv$startTime, pcmetaEnv$endTime, rrmetaEnv$startTime, rrmetaEnv$endTime)

startTime <- as.POSIXct(min(envsT), format = "%d-%b-%y")
endTime <- as.POSIXct(max(envsT), format = "%d-%b-%y")
```


```{r}
# I think we decided to not do it this way?

# create aggregate dataset if it doesn't exist yet.

# if(!file.exists("./04_product/PAR_Aggregate_2021.csv")) {
#   ll = read.csv("./01_input/JRW______/04_product/mydatafile.csv")
#   pc = read.csv("./01_input/JRW______/04_product/mydatafile.csv")
#   rr = read.csv("./01_input/JRW______/04_product/mydatafile.csv")
#   
#   sites = list(ll, pc, rr)
#   
#   agg = data.table::rbindlist(sites, use.names = TRUE)
#   
#   write.csv(agg, "./04_product/PAR_Aggregate_2021.csv")
# }
```


# Dataset Title
<!--     (be descriptive, more than 5 words.  Include what, where, when.) -->

```{r}
dataset.title <- "Measures of Photosynthetiaclly Active Radiation from 3 sites on the Judith River (Montana, USA) between March-November 2021."
```

**`r dataset.title`**


# Abstract
<!-- Modify /03_incremental/meta/abstract.md file to include what, where, when, why (brief), and how (brief methods).
-->


```{r}

# Load the abstract from the markdown file
# and copy the file to the EML directory

abstract <- readtext::readtext(
  file = "./03_incremental/meta/abstract.md",
  verbosity = 0
);
file.copy(
  from = "./03_incremental/meta/abstract.md",
  to = sprintf(
    "%s/abstract.md",
    emlalPath
  ),
  overwrite = TRUE
);

```

`r abstract$text`

# Purpose
<!-- Purpose should include a clear statement of the scholarly rationale for 
collecting this data set.  Historical to present statements (e.g., related to 
contamination, pollution, restoration, etc.) may be appropriate to 
define the context of the site and/or hypotheses.  Other contextual information
of interest includes reference to the general nature of associated data sets, 
the intention of any attendant funding, and the motivation for continued 
collection.
-->


```{r}

# Load the purpose text from the file.

purposeText <- readtext::readtext(
  file = "./03_incremental/meta/purpose.txt"
)

```

`r purposeText$text `

# Creators

<!-- Consider both attribution and responsibility.
Define contributor roles using the Contributor Roles Taxonomy where possible:
https://credit.niso.org/.
  Project title, fundingAgency and fundingNumber will remain empty until
  integrated into the personnel table in the Funding of this work section.
-->


```{r}
# Load the table of project people
#   (First step: modify people.xlsx to include all contributors to this data set (not a master list).
#   list INVESTIGATORS in people.xlsx in order as for a paper with e-mail addresses, organization 
#   and preferably ORCID ID, 
#   if you don’t have one, get it, it’s easy and free: http://orcid.org/). Add table rows as needed.

people <- as.data.frame(
  readxl::read_excel(
    path = "./01_input/meta/people_jrw.xlsx",
    sheet = "Personnel",
    range = readxl::cell_limits(ul = c(2, 2)),
    col_names = TRUE,
    col_types = rep("text", times = 7)
  )
)

rownames(people) <- people$internalUniqueID;
people <- people[,2:ncol(people)];

people[is.na(people)] <- "";

people$role <- "";
people$projectTitle <- "";
people$fundingAgency <- "";
people$fundingNumber <- "";
```

```{r}
# Define rows of the personnel table for creators

personnel <- people["PaynRob",]
rownames(personnel) <- "PaynCreator"
personnel["PaynCreator",]$role <- "creator"

personnel["FosterCreator",] <- people["FosterMadison",]
personnel["FosterCreator",]$role <- "creator"

# Specify the primary contact in the personnel table

personnel["PaynContact",] <- people["PaynRob",]
personnel["PaynContact",]$role <- "contact"

# Create the table for creators in this document

out <- data.frame(
  personnel$givenName,
  personnel$middleInitial,
  personnel$surName,
  personnel$organizationName,
  personnel$electronicMailAddress,
  personnel$ORCID
)

names(out) <- c(
  "First Name",
  "Middle Initial",
  "Last Name",
  "Organization",
  "e-mail address",
  "ORCID"
)

out <- out[1:(nrow(personnel) - 1),]

```

Rob Payn is the primary contact of rthis dataset.

`r knitr::kable(out)

`
# Other personnel names and roles

<!--
   Define contributor roles using the Contributor Roles Taxonomy where possible: https://credit.niso.org/.
-->


```{r}
other <- people["EwingStephanie",]
rownames(other) <- "Ewing"
other["Ewing",]$role <- "TBD"

other["Bayrd",] <- people["BayrdVenice",]
other["Bayrd",]$role <- "Data curation"

other["Hunter",] <- people["HunterShale",]
other["Hunter",]$role <- "Data curation"

# Madison's scripts mentioned Sean Williams at some point - is this an undergrad, grad student or what? Need to be included here or no?


# Create the table for other personnel for this document

other <- data.frame(
  other$givenName,
  other$middleInitial,
  other$surName,
  other$organizationName,
  other$electronicMailAddress,
  other$ORCID,
  other$role
)

names(other) <- c(
  "First Name",
  "Middle Initial",
  "Last Name",
  "Organization",
  "e-mail address",
  "ORCID",
  "Role in Project"
)
```

`r knitr::kable(other)`


# License

<!-- 1st step: Modify 03_incremental/meta/intellectual_rights.md to specify 
file names for both the code license (required) and the data rights (optional).  
Otherwise text should remain as is.
-->


```{r}

# Load the license text file and copy to the 03_incremental/temp/EML directory

license <- readtext::readtext(
  file = "./03_incremental/meta/intellectual_rights.md",
  verbosity = 0
)

file.copy(
  from = "./03_incremental/meta/intellectual_rights.md",
  to = sprintf(
    "%s/intellectual_rights.txt", 
    emlalPath
  ),
  overwrite = TRUE
)

```

`r license$text `

# Keywords
<!-- Use a controlled vocabulary to select up to five terms to fill the first 
five keyword slots.  Two good options include:
:: LTER Controlled Vocabulary: https://vocab.lternet.edu/vocab/vocab/index.php
:: NAL Agricultural Thesaurus and Glossary (NALT): https://agclass.nal.usda.gov/

Additionally, determine up to four keywords that best describe your lab, station, and/or project (e.g., Trout Lake Station, NTL LTER). This will help others discover your data by site/project.  

The final keyword(s) for CREWS or LTREB funded projects should be the primary grant award number(s):
CREWS, insert: "NSF OIA 1757351"
LTREB, insert: "NSF DEB LTREB 1655197"
Use both if they both apply.
-->

```{r}

# Create the keywords table

keywords <- rbind.data.frame(
  list(keyword = "photosynthetically active radiation", keywordThesaurus = "LTER Controlled Vocabulary"),
  list(keyword = "concentration", keywordThesaurus = "LTER Controlled Vocabulary"),
  list(keyword = "long term monitoring", keywordThesaurus = "LTER Controlled Vocabulary"),
  list(keyword = "water quality", keywordThesaurus = "LTER Controlled Vocabulary"),
  list(keyword = "Judith River", keywordThesaurus = ""),
  list(keyword = "Montana", keywordThesaurus = ""),
  list(keyword = "Northwestern United States", keywordThesaurus = ""),
  list(keyword = "Northwestern Forested Mountains Ecoregion", keywordThesaurus = ""),
  list(keyword = "NSF DEB LTREB 1655197", keywordThesaurus = "")
);

# Function to generate a markdown character string with
# the keywords grouped by thesaurus

printKeywords <- function(keywords)
{
  kw <- split(x = keywords, f = keywords$keywordThesaurus);
  names(kw)[names(kw) == ""] <- "No thesaurus";
  kwString <- "";
  for(name in names(kw)) {
    kwString <- paste0(
      kwString,
      "**", name, ":** ",
      paste(kw[[name]]$keyword, collapse = ", "),
      " \n\n"
    );
  }
  return(kwString);
}

```

`r printKeywords(keywords) `


```{r}

# Write the keywords table to the 03_incremental/temp/EML directory

write.table(
  x = keywords,
  file = sprintf(
    "%s/keywords.txt",
    emlalPath
  ),
  sep = "\t",
  row.names = FALSE,
  quote = FALSE,
  na = ""
)

```

# Funding of this work
<!-- 
  :: First step: Determine whether funding is for LTREB, CREWS, or both.  Comment out/
  add in the correct sources.  If Venice is included (for curation) in other personnel, CREWS
  should be included.
  
  -->

```{r}
funding <- as.data.frame(
  readxl::read_excel(
    path = "./01_input/meta/people_jrw.xlsx",
    sheet = "Funding",
    range = readxl::cell_limits(ul = c(2, 2)),
    col_names = TRUE,
    col_types = rep("text", times = 10)
  )
)

rownames(funding) <- funding$internalUniqueID
funding <- funding[,2:ncol(funding)]

funding[is.na(funding)] <- ""
funding$role <- "CREWS Project PI"

```

```{r}
# Add the funding to the personnel table

personnel <- rbind(personnel, funding)

# Create the table for funding personnel for this document

funding <- data.frame(
  funding$givenName,
  funding$middleInitial,
  funding$surName,
  funding$ORCID,
  funding$projectTitle,
  funding$fundingAgency,
  funding$fundingNumber
)

names(funding) <- c(
  "PI First Name",
  "PI Middle Initial",
  "PI Last Name",
  "PI ORCID",
  "Title of Grant",
  "Funding Agency",
  "Funding Identification Number"
)
```


```{r}

# Write the personnel table to the EML directory (includes creators, other personnel, funding)

write.table(
  x = personnel,
  file = sprintf(
    "%s/personnel.txt",
    emlalPath
  ),
  sep = "\t",
  row.names = FALSE,
  quote = FALSE,
  na = ""
)

```

`r knitr::kable(funding)`

```{r}
# Write the personnel table to the EML directory

write.table(
  x = personnel,
  file = sprintf(
    "%s/personnel.txt",
    emlalPath
  ),
  sep = "\t",
  row.names = FALSE,
  quote = FALSE,
  na = ""
)
```


# Timeframe
<!-- Content here is pulled from Global Environment metaEnv object.  No need to 
modify anything here, only check the html to verify that the correct dates are 
represented once run.
-->

```{r}

maintenance.description <- "Completed: Updates to these data are not expected"

```

* Begin date: `r format(metaEnv$startTime, format = "%e %B %Y")`, Date code: `r format(metaEnv$startTime, format = "%Y%m%d")`
* End date: `r format(metaEnv$endTime, format = "%e %B %Y")`, Date code: `r format(metaEnv$endTime, format = "%Y%m%d")`
* `r maintenance.description `


# Geographic location
<!-- Data content standard: All coordinates should be recorded in decimal degrees.

Verbal descriptions may be presented in markdown text. Otherwise, content in this code chunk is 
pulled from Global Environment objects for each site.
-->

Include verbal description of geographic location here: "Three sites..."




```{r}
geoCoverage <- rbind(llmetaEnv$location, pcmetaEnv$location, rrmetaEnv$location)

# Write the geographical coverage table to the EML directory

write.table(
  x = geoCoverage,
  file = sprintf(
    "%s/geographic_coverage.txt",
    emlalPath
  ),
  sep = "\t",
  row.names = FALSE,
  quote = FALSE,
  na = ""
)

```

# Methods

<!-- 
Code Notes:
Methods code below does not require editing beyond confirming the file path
of methods.md.  

Content Standard Notes:
methods.md requires extensive editing, particularly to pair file 
names with file descriptions. Third order and lower files do not need to be 
named/described unless the data product makes extensive use of lower order hierarchical 
structures for essential data/metadata.

Methods narrative description should focus on how the data were collected/derived, 
and should be cited as appropriate.  Standard operating procedures (SOPs) are 
recommended for inclusion in 02_protocol or separate archive (e.g., protocols.io), 
and are to be described in this section regardless of repository.  All derived 
data products must cite source data throughout the full chain of provenance, and 
include database search parameters sufficient to recreate the derived dataset when 
retrieving a subset from a larger database.
-->

```{r}

methods <- readtext::readtext(
  file = "./03_incremental/meta/methods.md",
  verbosity = 0
);
file.copy(
  from = "./03_incremental/meta/methods.md",
  to = sprintf(
    "%s/methods.md",
    emlalPath
  ),
  overwrite = TRUE
)

```

`r methods$text `

# Data tables and other entities

<!--The following code chunks (extending until the comment titled "Non-csv files at resource 
root level") are for csv files at resource root level only.  For example, in this
data product, these chunks apply only to AnnualAggregateFile-NH4-SRP-NO3-WY2019.csv.

Code Notes and Content Standards: (not updated for JRW)
Confirm the variable names of the "longmeta" object for both functions (tableDoc, tableAtts) match the column headers in the file 01_input/meta/attributes.xlsx.  Prefer to make changes to attributes.xlsx column headers rather than the longmeta variable names for consistency. Any changes made to attributes.xlsx first require re-run of 01_processing.Rmd prior to re-running this file (10_metadata.Rmd); prefer instead to run 99_runall.R.  Before re-running 01_processing / 99_runall, confirm within 01_processing that the filepath to attributes.xlsx is still correct. Next, spot check longmeta against attributes.xlsx again to confirm that the most current values are correctly transferring from file to object.  Confirm that 03_incremental/temp/metadata.Rdata is updating when running 01_processing.Rmd / 99_runall.R.

-->

```{r}
# function to display metadata in pretty table format

tableDoc <- function(longmeta) {
  
  df <- data.frame(
    sprintf(
      fmt = "%s. Property: %s. Entity: %s. Method: %s.",
      longmeta$Desc,
      longmeta$Property,
      longmeta$Entity,
      longmeta$Method
    ),
    longmeta$Units,
    paste(longmeta$EMLUnits, longmeta$EMLTimeFormat),
    longmeta$EMLClass,
    longmeta$EMLMissingCode,
    longmeta$EMLMissingExp,
    row.names = rownames(longmeta)
  );
  names(df) <- c(
    "Description",
    "Units",
    "EML Units or Time Format",
    "EML Class",
    "Missing value code",
    "Missing value code explanation"
  );
  return(df)
  
}

# function to add metadata to EML

tableAtt <- function(longmeta, tableName, emlalPath) {

  attributes <- data.frame(
    attributeName = rownames(longmeta),
    attributeDefinition = sprintf(
      fmt = "%s. Property: %s. Entity: %s. Method: %s.",
      longmeta$Desc,
      longmeta$Property,
      longmeta$Entity,
      longmeta$Method
    ),
    class = longmeta$EMLClass,
    unit = longmeta$EMLUnits,
    dateTimeFormatString = longmeta$EMLTimeFormat,
    missingValueCode = longmeta$EMLMissingCode,
    missingValueCodeExplanation = longmeta$EMLMissingExp
  );

  write.table(
    x = attributes,
    file = sprintf(
      "%s/attributes_%s.txt",
      emlalPath,
      tableName
    ),
    sep = "\t",
    row.names = FALSE,
    quote = FALSE,
    na = ""
  )
    
}

```


```{r}

# Convert the column metadata to long form

# All site tables are assumed to have the same column metadata, so only one version is used here

longmeta <- as.data.frame(t(as.matrix(llmetaEnv$tablePAR.meta)))

```

`r knitr::kable(tableDoc(longmeta))`


<!--
In data products with multiple data entities, the function tableAtt() must be run for each data entity (.csv file) in order to properly populate the EML. This also requires that the metaEnv object for each site is saved and named separately so that they can be called uniquely.

-->

```{r}


tableAtt(
  longmeta = longmeta, 
  tableName = llmetaEnv$tablePAR.name, # these metaEnvs haven't been named/created yet
  emlalPath = emlalPath
)

tableAtt(
  longmeta = longmeta, 
  tableName = pcmetaEnv$tablePAR.name, 
  emlalPath = emlalPath
)

tableAtt(
  longmeta = longmeta, 
  tableName = rrmetaEnv$tablePAR.name, 
  emlalPath = emlalPath
)
```

<!--Non-csv files at resource root level
The following chunk (extending until the section titled "Zip the files") is for non-csv files at resource root level only.  For example, in this data product, this chunk applies only to the soon-to-be-generated *.zip files (generated from root level directories "01_..." - "03_..." only), and to AnnualAggregateFile-NH4-SRP-NO3-WY2019_vis.pdf.

Confirm "otherNames" file names match with the soon-to-be-generated *.zip and non-csv files only.  "otherDescs" descriptions should match in respective order with the "otherNames" entries.

-->
```{r}

otherNames <- c(
  "01_input.zip",
  "02_protocol.zip",
  "03_incremental.zip"
);
otherDescs <- c(
  paste(
    "Compressed data pipeline folder containing original data files, in addition to code and notes for site-level preprocessing.",
   # "project default metadata.",
    "See methods for details."
  ),
  paste(
    "Compressed data pipeline folder containing",
  #  "protocols and SOPs, and",
    "R markdown and R scripts",
    "for processing data and metadata.",
    "See methods for details."
  ),
  paste(
    "Compressed data pipeline folder containing detailed notes",
    "associated with generation of the data product.",
    "See methods for details."
  )
)

```

`r paste("\n\n#### **Other file name:** ", otherNames, "\n\n#### **Other file description:**", otherDescs) `

# Zip the pipeline

```{r}
# Customized zip function for capturing output (doctored version of utils::zip)

customZip <- function (
  zipfile, files, flags = "-r9X", 
  extras = "", zip = Sys.getenv("R_ZIPCMD", "zip"), 
  stdout = TRUE, stderr = TRUE
) 
{
  if (missing(flags) && (!is.character(files) || !length(files))) 
    stop("'files' must be a character vector specifying one or more filepaths")
  if (!is.character(zip) || length(zip) != 1L || !nzchar(zip)) 
    stop("argument 'zip' must be a non-empty character string")
  args <- c(flags, shQuote(path.expand(zipfile)), shQuote(files), 
            extras)
  if (sum(nchar(c(args, Sys.getenv()))) + length(args) > 8000) {
    args <- c(flags, "-@", shQuote(path.expand(zipfile)), 
              extras)
    input <- files
  }
  else input <- NULL
  if (.Platform$OS.type == "windows") 
    invisible(system2(command = zip, args = args, input = input, invisible = TRUE, stdout = stdout, stderr = stderr))
  else invisible(system2(zip, args, input = input, stdout = stdout, stderr = stderr))
}
```

Create the pipeline zip files. Warnings with status 12 are normal if the zip file already exists and no files need to be freshened.

### 01_input.zip

```{r results = "asis"}
exclusions <- c(
  "./01_input/temp/*", 
  "./01_input/.temp/*",
  "./01_input/*/03_incremental/temp/",
  "./01_input/*/02_protocol/.Rproj.user/*",
  "./01_input/*/02_protocol/.Rhistory/*"
#  "./01_input/*/03_incremental/metadata_summary.html"
)
if (length(exclusions) > 0) {
  cat("*This pipeline directory is zipped with the following paths excluded:*\n\n")
  cat(
    paste(
      gsub("[*]", "\\\\*", exclusions),
      collapse = "  \n"
    )
  )
} 
```

```{r results = "asis"}
output <- customZip(
  zipfile = "./04_product/01_input.zip",
  files = c(
    "./01_input"
  ),
  flags = ifelse(
    file.exists("./04_product/01_input.zip"), 
    "-r9u", 
    "-r9"
  ),
  extras = paste(
    "-x",
    paste(shQuote(exclusions), collapse = " ")
  )
)
if (length(output) > 0) {
  cat("*Output from call to zip:* \n\n")
  cat(
    paste(output, collapse = "  \n")
  )
} 
```

### 02_protocol.zip:

```{r results = "asis"}
exclusions <- c(
  "./02_protocol/temp/*", 
  "./02_protocol/.temp/*",
  "./02_protocol/.Rproj.user/*",
  "./02_protocol/.Rhistory/*"
)
if (length(exclusions) > 0) {
  cat("*This pipeline directory is zipped with the following paths excluded:*\n\n")
  cat(
    paste(
      gsub("[*]", "\\\\*", exclusions),
      collapse = "  \n"
    )
  )
} 
```

```{r results = "asis"}
output <- customZip(
  zipfile = "./04_product/02_protocol.zip",
  files = c(
    "./02_protocol"
  ),
  flags = ifelse(
    file.exists("./04_product/02_protocol.zip"), 
    "-r9u", 
    "-r9"
  ),
  extras = paste(
    "-x",
    paste(shQuote(exclusions), collapse = " ")
  )
)
if (length(output) > 0) {
  cat("*Output from call to zip:* \n\n")
  cat(
    paste(output, collapse = "  \n")
  )
} 
```

### 03_incremental.zip:

```{r results = "asis"}
exclusions <- c(
  "./03_incremental/temp/*", 
  "./03_incremental/.temp/*",
  "./03_incremental/metadata_summary.html"
)
if (length(exclusions) > 0) {
  cat("*This pipeline directory is zipped with the following paths excluded:*\n\n")
  cat(
        paste(
      gsub("[*]", "\\\\*", exclusions),
      collapse = "  \n"
    )
  )
} 
```

```{r results = "asis"}
output <- customZip(
  zipfile = "./04_product/03_incremental.zip",
  files = c(
    "./03_incremental"
  ),
  flags = ifelse(
    file.exists("./04_product/03_incremental.zip"), 
    "-r9u", 
    "-r9"
  ),
  extras = paste(
    "-x",
    paste(shQuote(exclusions), collapse = " ")
  )
)
if (length(output) > 0) {
  cat("*Output from call to zip:* \n\n")
  cat(
    paste(output, collapse = "  \n")
  )
} 
```


# Test build of EML file

```{r}

if (exists("customUnits")) {
  
  write.table(
    x = customUnits,
    file = sprintf(
      "%s/custom_units.txt",
      emlalPath
    ),
    sep = "\t",
    row.names = FALSE,
    quote = FALSE,
    na = ""
  )
  
}

customUnitsOut <- function() {
  if(exists("customUnits")) {
    return(
      c(
        "Custom units table: <BR><BR>",
        knitr::kable(customUnits, row.names = FALSE, align = "l")
      )
    );
  }
}

```

`r if(exists("customUnits")) { "Custom units table: <BR><BR>"} `
`r if(exists("customUnits")) { knitr::kable(customUnits, row.names = FALSE, align = "l") } `

Output from the call to EMLassemblyline::make_eml:

```{r}

package.id <- "edi.945.1" # UPDATED FOR PAR 2021

emlFilePath <- sprintf(
  "./04_product/%s.xml",
  package.id
);

makeEML <- function() {
  EMLassemblyline::make_eml(
    path = emlalPath,
    data.path = "./04_product",
    eml.path = "./04_product", 
    dataset.title = dataset.title, 
    temporal.coverage = c(
      format(metaEnv$startTime, "%Y-%m-%d"), 
      format(metaEnv$endTime, "%Y-%m-%d")
    ), 
    maintenance.description = maintenance.description, 
   # data.table = tableName, 
   # data.table.description = tableDescs,
   # data.table.quote.character = rep("\"", times = length(tableName)),
  ##-above in orig script; this line not in original script: tableName = metaEnv$tableNH4SRP.name -- temp here for testing
  data.table = sprintf(
      "%s.csv",
      c(
        metaEnv$tablePAR.name
      )
    ), 
    data.table.description = c(
        metaEnv$tablePAR.desc
    ),
    data.table.quote.character = rep("\"", times = 9),
  ## FIXME -- what does times equal here? --##
  ## -- END changes to original script 20220519---
    other.entity = otherNames,
    other.entity.description = otherDescs,
    user.id = "UCFRResearch", # looks like this shouldn't be a problem for non-UCFR datasets
    user.domain = "EDI", 
    package.id = package.id
  );
  
  # Read the eml file
  
  emlxml <- xml2::read_xml(x = emlFilePath);
  
  # Replace intellectual rights with markdown element
  
  intRights <- xml2::xml_find_first(emlxml, ".//dataset/intellectualRights");
  
  markdown <- intRights %>% 
    xml2::xml_replace("intellectualRights") %>% 
    xml2::xml_add_child("markdown");
  xml2::xml_text(markdown) <- gsub(
    pattern = "\n\n", 
    replacement = "\r\n\r\n", 
    license$text
  );
  
  # Add the purpose element
  
  coverage <- xml2::xml_find_first(emlxml, ".//dataset/coverage");
  purpose <- coverage %>% xml2::xml_add_sibling("purpose") %>% xml2::xml_add_child("markdown");
  xml2::xml_text(purpose) <- purposeText$text;
  
  # Write the xml
  
  xml2::write_xml(emlxml, file = emlFilePath);
  
}

makeEML()

```

Output from the call to EMLassemblyline::issues: 

```{r}

EMLassemblyline::issues()

```

